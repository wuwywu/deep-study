{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、数据类型\n",
    "    int                     IntTensor\n",
    "    float                   FloatTensor\n",
    "    int array               IntTensor[d1,d2,...]\n",
    "    float array             FloatTensor[d1,d2,...]\n",
    "    string                  使用编码的方式表示string类型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from_numpy()\n",
    "使用 from_numpy() 将 numpy 数据转化为 tensor    \n",
    "默认为torch.float64类型 torch.DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.4000], dtype=torch.float64)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# import from numpy\n",
    "a = np.array([2,3.4])\n",
    "a = torch.from_numpy(a)\n",
    "print(a)\n",
    "b = np.ones([2,3])\n",
    "b = torch.from_numpy(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.2000]) torch.FloatTensor\n",
      "tensor([[2.0000, 3.2000],\n",
      "        [1.0000, 2.2000]]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# import from list\n",
    "a = torch.tensor([2., 3.2])\n",
    "print(a, a.type())\n",
    "b = torch.tensor([[2,3.2], [1,2.2]])\n",
    "print(b, b.type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# empty, Tensor, IntTensor, FloatTensor的应用\n",
    "torch.Tensor()是python类-----------torch.FloatTensor    \n",
    "torch.tensor()是python函数  \n",
    "Tensor()接收维度， tensor()接收数据     \n",
    "效果差不多，用法有一定的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 3.2000, 1.0000, 2.2000]]) torch.FloatTensor\n",
      "tensor([[1.4013e-45, 0.0000e+00, 1.4013e-45],\n",
      "        [0.0000e+00, 1.4013e-45, 0.0000e+00]]) torch.FloatTensor\n",
      "tensor([1, 2, 3]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据的函数\n",
    "a = torch.empty(1,4)\n",
    "print(a, a.type()) \n",
    "b = torch.Tensor(2,3)   # 随机\n",
    "print(b, b.type())\n",
    "c = torch.tensor((1,2,3))\n",
    "print(c, c.type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改变默认数据类型 \n",
    "PyTorch的默认类型是FloatTensor  \n",
    "使用set_default_tensor_type()改变默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1.2, 3]).type())\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.tensor([1.2, 3]).type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 6])\n",
      "torch.Size([2, 5, 6])\n",
      "tensor([0.6936, 0.9394, 0.9397, 0.9787, 0.9150, 0.8667, 0.5601, 0.9153, 0.7309,\n",
      "        0.9306, 0.6538, 0.9322, 0.8870, 0.5531, 0.7974, 0.7186, 0.8980, 0.9557,\n",
      "        0.9117, 0.8556, 0.9401, 0.5395, 0.8598, 0.7060, 0.9280, 0.5256, 0.5206,\n",
      "        0.5533, 0.5410, 0.9701, 0.6932, 0.5313, 0.9273, 0.8299, 0.5970, 0.6014,\n",
      "        0.7863, 0.9361, 0.9103, 0.6167, 0.8537, 0.8245, 0.9504, 0.6056, 0.7900,\n",
      "        0.9390, 0.5073, 0.7526, 0.9440, 0.8614, 0.5380, 0.5162, 0.5357, 0.7737,\n",
      "        0.6298, 0.7770, 0.6930, 0.9038, 0.9779, 0.5127, 0.8271, 0.5641, 0.5185,\n",
      "        0.9992, 0.6761])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,5,6)\n",
    "print(a.shape)\n",
    "b = a[0:3:2,:,:]\n",
    "print(b.shape)\n",
    "print(a[a>0.5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 维度变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2455, 0.2880],\n",
      "         [0.2440, 0.8290],\n",
      "         [0.1475, 0.2931]],\n",
      "\n",
      "        [[0.9267, 0.8703],\n",
      "         [0.6951, 0.8969],\n",
      "         [0.9060, 0.5716]],\n",
      "\n",
      "        [[0.9045, 0.0141],\n",
      "         [0.0079, 0.7683],\n",
      "         [0.2523, 0.5554]],\n",
      "\n",
      "        [[0.9802, 0.7401],\n",
      "         [0.8259, 0.2694],\n",
      "         [0.5085, 0.3293]]])\n",
      "tensor([[0.2455, 0.2880, 0.2440, 0.8290, 0.1475, 0.2931],\n",
      "        [0.9267, 0.8703, 0.6951, 0.8969, 0.9060, 0.5716],\n",
      "        [0.9045, 0.0141, 0.0079, 0.7683, 0.2523, 0.5554],\n",
      "        [0.9802, 0.7401, 0.8259, 0.2694, 0.5085, 0.3293]])\n"
     ]
    }
   ],
   "source": [
    "# 使用view变换维度\n",
    "a = torch.rand(4,3,2)\n",
    "print(a)\n",
    "\n",
    "b = a.view(4, 3*2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3, 2])\n",
      "torch.Size([4, 1, 3, 2])\n",
      "torch.Size([4, 3, 2, 1])\n",
      "torch.Size([4, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 增维\n",
    "# unsqueeze(x) 再x位置上插入一个维度\n",
    "print(a.unsqueeze(0).shape)\n",
    "print(a.unsqueeze(1).shape)\n",
    "print(a.unsqueeze(-1).shape)\n",
    "print(a.unsqueeze(-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1, 1])\n",
      "torch.Size([4, 3, 1, 1])\n",
      "torch.Size([4, 3, 1])\n",
      "torch.Size([4, 3, 1])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 降维(不包含参数的维度)\n",
    "a = torch.rand(4,3,1,1)\n",
    "print(a.squeeze(0).shape)\n",
    "print(a.squeeze(1).shape)\n",
    "print(a.squeeze(2).shape)\n",
    "print(a.squeeze(3).shape)\n",
    "print(a.squeeze(2).squeeze(2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7489],\n",
      "         [0.4490]],\n",
      "\n",
      "        [[0.6514],\n",
      "         [0.5683]],\n",
      "\n",
      "        [[0.1570],\n",
      "         [0.5956]],\n",
      "\n",
      "        [[0.6240],\n",
      "         [0.0187]]])\n",
      "tensor([[[0.7489, 0.7489],\n",
      "         [0.4490, 0.4490]],\n",
      "\n",
      "        [[0.6514, 0.6514],\n",
      "         [0.5683, 0.5683]],\n",
      "\n",
      "        [[0.1570, 0.1570],\n",
      "         [0.5956, 0.5956]],\n",
      "\n",
      "        [[0.6240, 0.6240],\n",
      "         [0.0187, 0.0187]]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 expand 扩展维度(扩张维度为1的位置)\n",
    "a = torch.rand(4,2,1)\n",
    "print(a)\n",
    "print(a.expand(4,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0964],\n",
      "        [0.7904]])\n",
      "tensor([[0.0964, 0.0964],\n",
      "        [0.7904, 0.7904],\n",
      "        [0.0964, 0.0964],\n",
      "        [0.7904, 0.7904]])\n",
      "tensor([[0.0964, 0.0964],\n",
      "        [0.7904, 0.7904],\n",
      "        [0.0964, 0.0964],\n",
      "        [0.7904, 0.7904],\n",
      "        [0.0964, 0.0964],\n",
      "        [0.7904, 0.7904],\n",
      "        [0.0964, 0.0964],\n",
      "        [0.7904, 0.7904]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 repeat 扩展维度\n",
    "a = torch.rand(2,1)\n",
    "print(a)\n",
    "print(a.repeat(2,2))\n",
    "print(a.repeat(2*2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼接与拆分 --- Cat, Stack, Split, Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9188, 0.3542],\n",
      "        [0.5319, 0.3193]]) tensor([[0.9521, 0.8719],\n",
      "        [0.5709, 0.3198]])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "(tensor([[0.9188],\n",
      "        [0.5319]]), tensor([[0.3542],\n",
      "        [0.3193]]))\n",
      "(tensor([[0.9188, 0.3542]]), tensor([[0.5319, 0.3193]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)\n",
    "print(a, b)\n",
    "print(torch.cat([a, b], dim=1).shape)\n",
    "print(torch.cat([a, b], dim=0).shape)\n",
    "print(torch.stack([a, b], dim=0).shape)\n",
    "print(torch.stack([a, b], dim=1).shape)\n",
    "print(a.split(1, dim=1))\n",
    "print(a.chunk(2, dim=0))    # 等量分割"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 的基本数学运算\n",
    "## 数学运算\n",
    "Add/minus/multiply/divide   \n",
    "Matmul  \n",
    "Pow     \n",
    "Sqrt/rsqrt  \n",
    "Round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2994, 0.2528, 0.8189, 0.1437],\n",
      "        [0.2561, 0.6133, 0.6274, 0.5532],\n",
      "        [0.0713, 0.0568, 0.1305, 0.3492]]) \n",
      " tensor([0.2602, 0.2979, 0.0097, 0.1281])\n",
      "tensor([[0.5596, 0.5507, 0.8286, 0.2717],\n",
      "        [0.5163, 0.9112, 0.6371, 0.6813],\n",
      "        [0.3314, 0.3547, 0.1402, 0.4773]])\n",
      "tensor([[0.5596, 0.5507, 0.8286, 0.2717],\n",
      "        [0.5163, 0.9112, 0.6371, 0.6813],\n",
      "        [0.3314, 0.3547, 0.1402, 0.4773]])\n"
     ]
    }
   ],
   "source": [
    "# add 加\n",
    "a = torch.rand(3,4)\n",
    "b = torch.rand(4)\n",
    "print(a, \"\\n\", b)\n",
    "print(a+b)\n",
    "print(torch.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0392, -0.0451,  0.8092,  0.0156],\n",
      "        [-0.0041,  0.3154,  0.6177,  0.4251],\n",
      "        [-0.1889, -0.2411,  0.1208,  0.2211]])\n",
      "tensor([[0.0779, 0.0753, 0.0079, 0.0184],\n",
      "        [0.0666, 0.1827, 0.0061, 0.0709],\n",
      "        [0.0185, 0.0169, 0.0013, 0.0447]])\n",
      "tensor([[ 1.1505,  0.8486, 84.3602,  1.1217],\n",
      "        [ 0.9844,  2.0586, 64.6309,  4.3194],\n",
      "        [ 0.2738,  0.1906, 13.4455,  2.7264]])\n"
     ]
    }
   ],
   "source": [
    "# sub 减；mul 乘；div 除\n",
    "print(torch.sub(a,b))\n",
    "print(torch.mul(a,b))\n",
    "print(torch.div(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(a-b, torch.sub(a,b))\n",
    "torch.all(torch.eq(a-b, torch.sub(a,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0767]])\n",
      "tensor([[1.0767]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵相乘 matmul --- 另一种写法：@\n",
    "a = torch.rand(1,4)\n",
    "b = torch.rand(4,1)\n",
    "print(torch.matmul(a,b))\n",
    "print(a@b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 9, 9],\n",
      "        [9, 9, 9]])\n",
      "tensor([[9, 9, 9],\n",
      "        [9, 9, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Power exp log\n",
    "# sqrt ,rsqrt\n",
    "a = torch.full([2,3],3)\n",
    "print(a.pow(2))\n",
    "print(a**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计性质\n",
    "norm----范数        \n",
    "mean_sum    \n",
    "prod----所有元素的积    \n",
    "max, min, argmin, argmax    \n",
    "kthvalue, topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float32)\n",
      "tensor(0., dtype=torch.float32) tensor(7., dtype=torch.float32) tensor(3.5000, dtype=torch.float32) tensor(0., dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(8).view(2,4).type(torch.FloatTensor)\n",
    "print(a)\n",
    "print(a.min(), a.max(), a.mean(), a.prod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "111111111\n"
     ]
    }
   ],
   "source": [
    "# 类中__call__的使用\n",
    "class test:\n",
    "    def __init__(self, a):\n",
    "        print(a)\n",
    "        pass\n",
    "    def __call__(self, b):\n",
    "        print(b)\n",
    "        pass\n",
    "c = test(1111)\n",
    "\n",
    "c(111111111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# where 和 gather操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2367, 0.7066],\n",
      "        [0.9486, 0.6572]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where\n",
    "# torch.where(condition, x, y)\n",
    "cond = torch.rand(2,2)\n",
    "print(cond)\n",
    "a = torch.ones(2,2)\n",
    "b = torch.zeros(2,2)\n",
    "torch.where(cond>0.5, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[101, 101, 102],\n",
       "        [101, 102, 103],\n",
       "        [101, 104, 103],\n",
       "        [100, 105, 103]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather搜集\n",
    "# torch.gather(input, dim, index, out=None)\n",
    "a = torch.arange(10)+100\n",
    "print(a)\n",
    "torch.gather(a.expand(4,10), dim=1, index=torch.tensor([[1,1,2],[1,2,3],[1,4,3],[0,5,3]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 自动求导\n",
    "1、创建求导变量，在创建张量时 requires_grad=True    \n",
    "2、使用 backward() 求出导数（梯度）或者使用 autograd.grad() 求出导数（梯度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([8.]),)\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "x = torch.ones(1)\n",
    "# w = torch.full([1],5., requires_grad=True)\n",
    "w = torch.full([1],5.)\n",
    "w.requires_grad_()\n",
    "mse = F.mse_loss(torch.ones(1), x*w)\n",
    "\n",
    "# 使用 autograd.grad() 求导\n",
    "# 使用 retain_graph 可保留图（每次求导，图被清除，必须再次建立，使用retain_graph可保留图到下次求导）\n",
    "a = torch.autograd.grad(mse, [w], retain_graph=True)\n",
    "print(a)\n",
    "\n",
    "# 使用 backward() 求导\n",
    "mse.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1, 11,  3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 11,  3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([-1,2,3])\n",
    "b=torch.tensor([3,2,3])\n",
    "a[1] = 11\n",
    "print(a)\n",
    "torch.where((a>1)&(b<3))\n",
    "\n",
    "torch.abs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
