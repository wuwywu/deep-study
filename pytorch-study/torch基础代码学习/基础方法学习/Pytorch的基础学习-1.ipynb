{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、数据类型\n",
    "    int                     IntTensor\n",
    "    float                   FloatTensor\n",
    "    int array               IntTensor[d1,d2,...]\n",
    "    float array             FloatTensor[d1,d2,...]\n",
    "    string                  使用编码的方式表示string类型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from_numpy()\n",
    "使用 from_numpy() 将 numpy 数据转化为 tensor    \n",
    "默认为torch.float64类型 torch.DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.4000], dtype=torch.float64)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# import from numpy\n",
    "a = np.array([2,3.4])\n",
    "a = torch.from_numpy(a)\n",
    "print(a)\n",
    "b = np.ones([2,3])\n",
    "b = torch.from_numpy(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.2000]) torch.FloatTensor\n",
      "tensor([[2.0000, 3.2000],\n",
      "        [1.0000, 2.2000]]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# import from list\n",
    "a = torch.tensor([2., 3.2])\n",
    "print(a, a.type())\n",
    "b = torch.tensor([[2,3.2], [1,2.2]])\n",
    "print(b, b.type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# empty, Tensor, IntTensor, FloatTensor的应用\n",
    "torch.Tensor()是python类-----------torch.FloatTensor    \n",
    "torch.tensor()是python函数  \n",
    "Tensor()接收维度， tensor()接收数据     \n",
    "效果差不多，用法有一定的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,         nan, -7.0128e+11,  8.9543e-43]]) torch.FloatTensor\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.9543e-43]]) torch.FloatTensor\n",
      "tensor([1, 2, 3]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据的函数\n",
    "a = torch.empty(1,4)\n",
    "print(a, a.type()) \n",
    "b = torch.Tensor(2,3)   # 随机\n",
    "print(b, b.type())\n",
    "c = torch.tensor((1,2,3))\n",
    "print(c, c.type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改变默认数据类型 \n",
    "PyTorch的默认类型是FloatTensor  \n",
    "使用set_default_tensor_type()改变默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1.2, 3]).type())\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.tensor([1.2, 3]).type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 6])\n",
      "torch.Size([2, 5, 6])\n",
      "tensor([0.5367, 0.6427, 0.6292, 0.8174, 0.7703, 0.9470, 0.5937, 0.6342, 0.6710,\n",
      "        0.8164, 0.5957, 0.7833, 0.5554, 0.5252, 0.8230, 0.6194, 0.7529, 0.9362,\n",
      "        0.8237, 0.8761, 0.9560, 0.6542, 0.6946, 0.8084, 0.6256, 0.7092, 0.9195,\n",
      "        0.9538, 0.8504, 0.9426, 0.8878, 0.6145, 0.7628, 0.9353, 0.6431, 0.5185,\n",
      "        0.5811, 0.6726, 0.9313, 0.9609, 0.9218, 0.9146, 0.9001, 0.8053, 0.8924,\n",
      "        0.8533, 0.8756, 0.9888, 0.7353, 0.9842, 0.5370, 0.7046, 0.5139, 0.9883,\n",
      "        0.8992, 0.6856, 0.6633, 0.8428, 0.9896, 0.5692, 0.9610, 0.9237, 0.7623,\n",
      "        0.8798, 0.6895, 0.6518])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,5,6)\n",
    "print(a.shape)\n",
    "b = a[0:3:2,:,:]\n",
    "print(b.shape)\n",
    "print(a[a>0.5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 维度变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8063, 0.1488],\n",
      "         [0.3338, 0.7083],\n",
      "         [0.4969, 0.2100]],\n",
      "\n",
      "        [[0.5345, 0.0873],\n",
      "         [0.2173, 0.2530],\n",
      "         [0.6445, 0.3866]],\n",
      "\n",
      "        [[0.8425, 0.2169],\n",
      "         [0.7613, 0.6000],\n",
      "         [0.9416, 0.1514]],\n",
      "\n",
      "        [[0.8783, 0.2111],\n",
      "         [0.6348, 0.1420],\n",
      "         [0.7341, 0.1306]]])\n",
      "tensor([[0.8063, 0.1488, 0.3338, 0.7083, 0.4969, 0.2100],\n",
      "        [0.5345, 0.0873, 0.2173, 0.2530, 0.6445, 0.3866],\n",
      "        [0.8425, 0.2169, 0.7613, 0.6000, 0.9416, 0.1514],\n",
      "        [0.8783, 0.2111, 0.6348, 0.1420, 0.7341, 0.1306]])\n"
     ]
    }
   ],
   "source": [
    "# 使用view变换维度\n",
    "a = torch.rand(4,3,2)\n",
    "print(a)\n",
    "\n",
    "b = a.view(4, 3*2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3, 2])\n",
      "torch.Size([4, 1, 3, 2])\n",
      "torch.Size([4, 3, 2, 1])\n",
      "torch.Size([4, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 增维\n",
    "# unsqueeze(x) 再x位置上插入一个维度\n",
    "print(a.unsqueeze(0).shape)\n",
    "print(a.unsqueeze(1).shape)\n",
    "print(a.unsqueeze(-1).shape)\n",
    "print(a.unsqueeze(-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1, 1])\n",
      "torch.Size([4, 3, 1, 1])\n",
      "torch.Size([4, 3, 1])\n",
      "torch.Size([4, 3, 1])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 降维(不包含参数的维度)\n",
    "a = torch.rand(4,3,1,1)\n",
    "print(a.squeeze(0).shape)\n",
    "print(a.squeeze(1).shape)\n",
    "print(a.squeeze(2).shape)\n",
    "print(a.squeeze(3).shape)\n",
    "print(a.squeeze(2).squeeze(2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4329],\n",
      "         [0.3081]],\n",
      "\n",
      "        [[0.9665],\n",
      "         [0.1837]],\n",
      "\n",
      "        [[0.4874],\n",
      "         [0.0620]],\n",
      "\n",
      "        [[0.8363],\n",
      "         [0.7169]]])\n",
      "tensor([[[0.4329, 0.4329],\n",
      "         [0.3081, 0.3081]],\n",
      "\n",
      "        [[0.9665, 0.9665],\n",
      "         [0.1837, 0.1837]],\n",
      "\n",
      "        [[0.4874, 0.4874],\n",
      "         [0.0620, 0.0620]],\n",
      "\n",
      "        [[0.8363, 0.8363],\n",
      "         [0.7169, 0.7169]]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 expand 扩展维度(扩张维度为1的位置)\n",
    "a = torch.rand(4,2,1)\n",
    "print(a)\n",
    "print(a.expand(4,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8846],\n",
      "        [0.2506]])\n",
      "tensor([[0.8846, 0.8846],\n",
      "        [0.2506, 0.2506],\n",
      "        [0.8846, 0.8846],\n",
      "        [0.2506, 0.2506]])\n",
      "tensor([[0.8846, 0.8846],\n",
      "        [0.2506, 0.2506],\n",
      "        [0.8846, 0.8846],\n",
      "        [0.2506, 0.2506],\n",
      "        [0.8846, 0.8846],\n",
      "        [0.2506, 0.2506],\n",
      "        [0.8846, 0.8846],\n",
      "        [0.2506, 0.2506]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 repeat 扩展维度\n",
    "a = torch.rand(2,1)\n",
    "print(a)\n",
    "print(a.repeat(2,2))\n",
    "print(a.repeat(2*2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼接与拆分 --- Cat, Stack, Split, Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9559, 0.8819],\n",
      "        [0.0077, 0.8526]]) tensor([[0.0281, 0.2573],\n",
      "        [0.1914, 0.9745]])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "(tensor([[0.9559],\n",
      "        [0.0077]]), tensor([[0.8819],\n",
      "        [0.8526]]))\n",
      "(tensor([[0.9559, 0.8819]]), tensor([[0.0077, 0.8526]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)\n",
    "print(a, b)\n",
    "print(torch.cat([a, b], dim=1).shape)\n",
    "print(torch.cat([a, b], dim=0).shape)\n",
    "print(torch.stack([a, b], dim=0).shape)\n",
    "print(torch.stack([a, b], dim=1).shape)\n",
    "print(a.split(1, dim=1))\n",
    "print(a.chunk(2, dim=0))    # 等量分割"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 的基本数学运算\n",
    "## 数学运算\n",
    "Add/minus/multiply/divide   \n",
    "Matmul  \n",
    "Pow     \n",
    "Sqrt/rsqrt  \n",
    "Round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7376, 0.5001, 0.3959, 0.5535],\n",
      "        [0.1764, 0.8908, 0.6034, 0.8045],\n",
      "        [0.7392, 0.7757, 0.7019, 0.6842]]) \n",
      " tensor([0.9988, 0.1284, 0.0111, 0.6083])\n",
      "tensor([[1.7364, 0.6285, 0.4069, 1.1618],\n",
      "        [1.1752, 1.0192, 0.6145, 1.4128],\n",
      "        [1.7381, 0.9041, 0.7130, 1.2925]])\n",
      "tensor([[1.7364, 0.6285, 0.4069, 1.1618],\n",
      "        [1.1752, 1.0192, 0.6145, 1.4128],\n",
      "        [1.7381, 0.9041, 0.7130, 1.2925]])\n"
     ]
    }
   ],
   "source": [
    "# add 加\n",
    "a = torch.rand(3,4)\n",
    "b = torch.rand(4)\n",
    "print(a, \"\\n\", b)\n",
    "print(a+b)\n",
    "print(torch.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2613,  0.3717,  0.3848, -0.0547],\n",
      "        [-0.8225,  0.7624,  0.5923,  0.1963],\n",
      "        [-0.2596,  0.6473,  0.6909,  0.0759]])\n",
      "tensor([[0.7367, 0.0642, 0.0044, 0.3367],\n",
      "        [0.1762, 0.1144, 0.0067, 0.4894],\n",
      "        [0.7384, 0.0996, 0.0078, 0.4162]])\n",
      "tensor([[ 0.7384,  3.8940, 35.7988,  0.9100],\n",
      "        [ 0.1766,  6.9363, 54.5637,  1.3227],\n",
      "        [ 0.7401,  6.0399, 63.4725,  1.1249]])\n"
     ]
    }
   ],
   "source": [
    "# sub 减；mul 乘；div 除\n",
    "print(torch.sub(a,b))\n",
    "print(torch.mul(a,b))\n",
    "print(torch.div(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(a-b, torch.sub(a,b))\n",
    "torch.all(torch.eq(a-b, torch.sub(a,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2245]])\n",
      "tensor([[0.2245]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵相乘 matmul --- 另一种写法：@\n",
    "a = torch.rand(1,4)\n",
    "b = torch.rand(4,1)\n",
    "print(torch.matmul(a,b))\n",
    "print(a@b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 9, 9],\n",
      "        [9, 9, 9]])\n",
      "tensor([[9, 9, 9],\n",
      "        [9, 9, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Power exp log\n",
    "# sqrt ,rsqrt\n",
    "a = torch.full([2,3],3)\n",
    "print(a.pow(2))\n",
    "print(a**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计性质\n",
    "norm----范数        \n",
    "mean_sum    \n",
    "prod----所有元素的积    \n",
    "max, min, argmin, argmax    \n",
    "kthvalue, topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float32)\n",
      "tensor(0., dtype=torch.float32) tensor(7., dtype=torch.float32) tensor(3.5000, dtype=torch.float32) tensor(0., dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(8).view(2,4).type(torch.FloatTensor)\n",
    "print(a)\n",
    "print(a.min(), a.max(), a.mean(), a.prod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "111111111\n"
     ]
    }
   ],
   "source": [
    "# 类中__call__的使用\n",
    "class test():\n",
    "    def __init__(self, a):\n",
    "        print(a)\n",
    "        pass\n",
    "    def __call__(self, b):\n",
    "        print(b)\n",
    "        pass\n",
    "c = test(1111)\n",
    "c(111111111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# where 和 gather操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8965, 0.8287],\n",
      "        [0.8277, 0.9551]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where\n",
    "# torch.where(condition, x, y)\n",
    "cond = torch.rand(2,2)\n",
    "print(cond)\n",
    "a = torch.ones(2,2)\n",
    "b = torch.zeros(2,2)\n",
    "torch.where(cond>0.5, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[101, 101, 102],\n",
       "        [101, 102, 103],\n",
       "        [101, 104, 103],\n",
       "        [100, 105, 103]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather搜集\n",
    "# torch.gather(input, dim, index, out=None)\n",
    "a = torch.arange(10)+100\n",
    "print(a)\n",
    "torch.gather(a.expand(4,10), dim=1, index=torch.tensor([[1,1,2],[1,2,3],[1,4,3],[0,5,3]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 自动求导\n",
    "1、创建求导变量，在创建张量时 requires_grad=True    \n",
    "2、使用 backward() 求出导数（梯度）或者使用 autograd.grad() 求出导数（梯度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([8.]),)\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "x = torch.ones(1)\n",
    "# w = torch.full([1],5., requires_grad=True)\n",
    "w = torch.full([1],5.)\n",
    "w.requires_grad_()\n",
    "mse = F.mse_loss(torch.ones(1), x*w)\n",
    "\n",
    "# 使用 autograd.grad() 求导\n",
    "# 使用 retain_graph 可保留图（每次求导，图被清除，必须再次建立，使用retain_graph可保留图到下次求导）\n",
    "a = torch.autograd.grad(mse, [w], retain_graph=True)\n",
    "print(a)\n",
    "\n",
    "# 使用 backward() 求导\n",
    "mse.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
